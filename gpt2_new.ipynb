{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMu2/X4/xP+rLdzP01I+8dU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Pmvmo0p4a2nu"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torchvision import datasets\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, random_split, Dataset\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision\n","from PIL import Image"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","with open('input.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OLXVyp3YeH6m","executionInfo":{"status":"ok","timestamp":1697565894250,"user_tz":240,"elapsed":300,"user":{"displayName":"Khanh Dong Minh","userId":"14329809154450868168"}},"outputId":"8a49810b-beeb-441a-e8ec-b4dd72359fe9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-10-17 18:04:53--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1115394 (1.1M) [text/plain]\n","Saving to: ‘input.txt’\n","\n","\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n","\n","2023-10-17 18:04:53 (48.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n","\n"]}]},{"cell_type":"code","source":["# here are all the unique characters that occur in this text\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","# create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n","decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"],"metadata":{"id":"Zqc6bkTZeIff"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data)) # first 90% will be train, rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# data loading\n","def get_batch(split):\n","    # generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out"],"metadata":{"id":"KIKsBG6-ekq_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameter\n"],"metadata":{"id":"FRNy973aL-yA"}},{"cell_type":"code","source":["batch_size = 64 # how many independent sequences will we process in parallel?\n","block_size = 256 # what is the maximum context length for predictions?\n","#vocab_size = 50304\n","max_iters = 3000\n","eval_interval = 500\n","learning_rate = 3e-4\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embd = 384\n","n_head = 6\n","n_layer = 6\n","dropout = 0.2"],"metadata":{"id":"pOEIOoQohRS3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Multi Heads Attention"],"metadata":{"id":"d6CujOn1MC0G"}},{"cell_type":"code","source":["class SelfAttention(nn.Module):\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embd, head_size)\n","        self.query = nn.Linear(n_embd, head_size)\n","        self.value = nn.Linear(n_embd, head_size)\n","\n","        #Buffers are persistent tensors that are part of the module's parameters but are not updated during backpropagation.\n","        #torch.tril create a lower triangular matrix for masking purposes\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        B,T,C = x.shape\n","        k = self.key(x)\n","        q = self.query(x)\n","\n","        #calculate attention score\n","        # k.shape[-1]**-0.5 --> scale the dot product by the result of square root of k\n","        attn = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5              # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n","        mask = self.tril[:T, :T] == 0\n","        #upper triangle is 0 so all the elements there will be true, and is replaced with -inf\n","        attn = attn.masked_fill(mask, float('-inf')) # (B, T, T)\n","        attn = F.softmax(attn, dim=-1)\n","        attn = self.dropout(attn)\n","\n","        # perform the weighted aggregation of the values\n","        v = self.value(x) # (B,T,hs)\n","        out = attn @ v #  (B, T, hs)\n","        return out\n"],"metadata":{"id":"JclO_OL_uk_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.headlist = nn.ModuleList([SelfAttention(head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(head_size * num_heads, n_embd)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        multihead = [h(x) for h in self.headlist]\n","        out = torch.cat(multihead, dim=-1)\n","\n","        #reduce dimension to n_embd\n","        out = self.proj(out)\n","        out = self.dropout(out)\n","        return out\n"],"metadata":{"id":"hrFBXCuWDsQ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Decoder Block"],"metadata":{"id":"z3VZpmWvMI6h"}},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, n_embd):\n","        super().__init__()\n","        self.c_fc    = nn.Linear(n_embd, 4 * n_embd)  #OpenAI use Conv1d, will that be better for image recognition purpose?\n","        self.gelu    = nn.GELU() # OpenAI use GELU\n","        self.c_proj  = nn.Linear(4 * n_embd, n_embd) #Residual Connection\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.c_fc(x)\n","        x = self.gelu(x)\n","        x = self.c_proj(x)\n","        x = self.dropout(x)\n","        return x\n"],"metadata":{"id":"sY9QLHkiGdSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Block(nn.Module):\n","    def __init__(self, n_embd, n_head):\n","        super().__init__()\n","        head_size = n_embd // n_head\n","        self.attn = MultiHeadAttention(n_head, head_size)\n","        self.mlp = MLP(n_embd)\n","        self.ln1 = nn.LayerNorm(n_embd)\n","        self.ln2 = nn.LayerNorm(n_embd)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln1(x))  #LayerNorm is applied b4 attention or feed forward layer, which is different from the 2017 paper\n","        x = x + self.mlp(self.ln2(x))   # add x for residual connection\n","        return x"],"metadata":{"id":"LxIenAexGv4O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# GPT2\n"],"metadata":{"id":"59K3Kef-SJ8l"}},{"cell_type":"code","source":["class GPT2(nn.Module):\n","   def __init__(self):\n","        super().__init__()\n","        self.wte = nn.Embedding(vocab_size, n_embd) # token embedding\n","        self.wpe = nn.Embedding(block_size, n_embd) # positional encoding\n","\n","        self.blocklist = [Block(n_embd, n_head=n_head) for _ in range(n_layer)]\n","        self.blocks = nn.Sequential(*self.blocklist) # '*' operator unpack iterables like lists/tuples\n","\n","        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n","        self.lm_head = nn.Linear(n_embd, vocab_size) # lm_head takes the output from the transformer's layers and projects it to the vocabulary size -> predict the next token\n","\n","        # better init, not covered in the original GPT video\n","        self.apply(self._init_weights)\n","\n","   def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","   def forward(self, idx, targets=None):\n","        B, T = idx.shape\n","\n","        # idx and targets are both (B,T) tensor of integers\n","        tok_emb = self.wte(idx) # (B,T,C)\n","        pos_emb = self.wpe(torch.arange(T, device=device)) # torch.arange generates a 1-dimensional tensor with values ranging from 0 to T-1.\n","        x = tok_emb + pos_emb # (B,T,C)\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x) # (B,T,vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","\n","        return logits, loss\n","\n","   def generate(self, idx, max_new_tokens):\n","        # idx is (B, T) array of indices in the current context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)\n","            # focus only on the last time step\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1) # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n","        return idx"],"metadata":{"id":"EAPe2Ao1SNs7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2()\n","m = model.to(device)\n","# print the number of parameters in the model\n","print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n","\n","# create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    # sample a batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIALqJank3Yb","executionInfo":{"status":"ok","timestamp":1697567839947,"user_tz":240,"elapsed":1913032,"user":{"displayName":"Khanh Dong Minh","userId":"14329809154450868168"}},"outputId":"9b6ed234-d158-47fa-ef82-5b6d680dbd1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["10.795841 M parameters\n","step 0: train loss 4.2241, val loss 4.2273\n","step 500: train loss 2.0079, val loss 2.0978\n","step 1000: train loss 1.5078, val loss 1.7021\n","step 1500: train loss 1.3400, val loss 1.5693\n","step 2000: train loss 1.2479, val loss 1.5146\n","step 2500: train loss 1.1757, val loss 1.4864\n","step 2999: train loss 1.1138, val loss 1.4748\n","\n","Yoru supreth him; a bridle king'd with his staff,\n","And you on of Sobtle's much her.\n","\n","TrON:\n","The low now is great hrim it Juliht.\n","\n","CATESBY:\n","And I am I many murdering.\n","\n","BENVOLIO:\n","'Tis amos on a crown, with such a man's cheeks\n","thoughts them not what's biddle my soldier about,\n","Ladiests of themrotst with being readies: if and so\n","He excellent torm me, or trouble page against him\n","White'er the breat of all revenge as when them who the gate;\n","The Frence we to misgow his is at this Viel here.\n","\n","First Soldier:\n"]}]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"St0L38_G84W-"}},{"cell_type":"code","source":["class GPT2ImageClassifier(nn.Module):\n","    def __init__(self, num_channels=3, num_classes=24):\n","        super(GPT2ImageClassifier, self).__init__()\n","\n","        # Convolutional layers\n","        if num_channels == 3:\n","            self.conv1 = nn.Conv2d(3, 16, 3, stride=1, padding=1) # input channel to 3 for RGB\n","        else:\n","            self.conv1 = nn.Conv2d(1, 16, 3, stride=1, padding=1) # input channel to 3 for RGB\n","\n","        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # GPT-2 configuration and model\n","        #self.config = GPT2Config()\n","        self.gpt2 = GPT2()\n","\n","        # Adjust the output size of the CNN to match GPT-2's expected input size\n","        self.fc1 = nn.Linear(64 * 16 * 16, n_embd)\n","\n","        # Classifier\n","        self.fc2 = nn.Linear(n_embd, num_classes)\n","\n","    def forward(self, x):\n","        # Convolutional layers\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","\n","        # Flatten the output\n","        x = x.view(x.size(0), -1)\n","        x = self.fc1(x)\n","\n","        # Reshape for GPT-2\n","        #x = x.unsqueeze(1)  # Add sequence length dimension\n","\n","        # GPT-2 model\n","        outputs = self.gpt2(inputs_embeds=x)\n","        x = outputs.last_hidden_state[:, 0, :]\n","\n","        # Classifier\n","        x = self.fc2(x)\n","\n","        return x"],"metadata":{"id":"z0nIuHu13yzd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainandsave(dataset, train_loader, validation_loader, num_channels, num_classes, name):\n","    # Instantiate the model\n","    model = GPT2ImageClassifier(num_channels=num_channels, num_classes=num_classes)\n","    model.to(device)\n","    model = torch.nn.DataParallel(model)\n","    cudnn.benchmark = True\n","\n","    # Define loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Train the model\n","    num_epochs = 5\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","        correct_train = 0\n","        total_train = 0\n","\n","        # Training loop\n","        for images, labels in train_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","            _, predicted_train = torch.max(outputs.data, 1)\n","            total_train += labels.size(0)\n","            correct_train += (predicted_train == labels).sum().item()\n","\n","        train_accuracy = 100 * correct_train / total_train\n","\n","        # Validation loop\n","        model.eval()\n","        correct_test = 0\n","        total_test = 0\n","        with torch.no_grad():\n","            for images, labels in validation_loader:\n","                images = images.to(device)\n","                labels = labels.to(device)\n","                outputs = model(images)\n","                _, predicted_test = torch.max(outputs.data, 1)\n","                total_test += labels.size(0)\n","                correct_test += (predicted_test == labels).sum().item()\n","\n","        test_accuracy = 100 * correct_test / total_test\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss/len(train_loader)}, \\\n","        Train Accuracy: {train_accuracy}%, Val Accuracy: {test_accuracy}%\")\n","\n","#     # Get some random test images\n","#     dataiter = iter(test_loader)\n","#     images, labels = next(dataiter)\n","\n","#     # Get the predicted labels\n","#     model.eval()  # Make sure the model is in evaluation mode\n","#     with torch.no_grad():\n","#         images = images.to(device)\n","#         outputs = model(images)\n","#         _, predicted = torch.max(outputs.data, 1)\n","#     # Convert tensor indices to actual class names\n","#     predicted_labels = [dataset.classes[i] for i in predicted]\n","\n","#     # Show images with the predicted labels as the title\n","#     imshow(torchvision.utils.make_grid(images), predicted_labels,'predictedimg.png')\n","    print(f\"Save {name}\")\n","    torch.save(model.state_dict(), name)"],"metadata":{"id":"cStn25C7HODI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def datasetload(path='./imagesample'):\n","    # Define image transformations\n","    transform = transforms.Compose([\n","        transforms.Resize((128, 128)),  # Resize images to match the input size of the model\n","        transforms.ToTensor(),\n","    ])\n","\n","    # Load the dataset\n","    dataset = ImageFolder(root=path, transform=transform)\n","\n","    # Get the number of classes\n","    num_classes = len(dataset.classes)\n","\n","    # Split the dataset into training and testing sets\n","    train_size = int(0.8 * len(dataset))\n","    test_size = len(dataset) - train_size\n","    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","    return dataset, train_dataset, test_dataset, train_loader, test_loader, num_classes"],"metadata":{"id":"K1ZtlwUYXnTq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainfunction(path):\n","    dataset1,train_dataset, test_dataset,train_loader,test_loader,num_classes = datasetload(path)\n","    trainandsave(dataset1,train_loader,test_loader,num_classes)\n","\n","trainfunction()"],"metadata":{"id":"bQmbwSiRX-jb"},"execution_count":null,"outputs":[]}]}